{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e736d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x - x_max)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1.0 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 *(x**3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3ecc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    def __init__(self, vocab_size, d_model, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.E = rng.standard_normal((vocab_size, d_model)) / np.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.E[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9725d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, max_len, d_model, mode='sinusoidal', seed=123):\n",
    "        self.mode = mode\n",
    "        self.max_len= max_len\n",
    "        self.d_model = d_model\n",
    "        if mode == 'sinusoidal':\n",
    "            self.P = self._get_sinusoidal_encoding(max_len, d_model)\n",
    "        elif mode == 'learned':\n",
    "            rng = np.random.default_rng(seed)\n",
    "            self.P = rng.standard_normal((max_len, d_model)) / np.sqrt(d_model)\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'sinusoidal' or 'learned'\")\n",
    "        \n",
    "    def _get_sinusoidal_encoding(self, max_len, d_model):  # Fixed: Added self parameter\n",
    "        pos = np.arange(max_len)[:, None]\n",
    "        i = np.arange(d_model)[None, :]\n",
    "        rates = 1 / np.power(10000, (2*(i//2)) / d_model)\n",
    "        angles = pos * rates\n",
    "        P = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "        P[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        P[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        return P\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, d = x.shape\n",
    "        assert d == self.d_model\n",
    "        return self.P[:T][None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8ea66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones((d_model,), dtype=np.float32)\n",
    "        self.beta = np.zeros((d_model,), dtype=np.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        var = ((x - mean)**2).mean(axis=-1, keepdims=True)\n",
    "        x_norm = (x - mean) / np.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41344e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_causal_mask(T):\n",
    "    mask = np.triu(np.ones((T, T), dtype=np.float32), k=1)\n",
    "    mask = np.where(mask==1, -1e9, 0.0)\n",
    "    return mask[None, None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7b3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        dh = Q.shape[-1]\n",
    "        scores = np.matmul(Q, K.transpose(0,1,3,2)) / np.sqrt(dh)\n",
    "        if mask is not None:\n",
    "            scores += mask\n",
    "        A = softmax(scores, axis=-1)\n",
    "        out = np.matmul(A, V)\n",
    "        return out, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10726e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention:\n",
    "    def __init__(self, d_model, num_heads, seed=7):\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.W_Q = rng.standard_normal((d_model, d_model)) / np.sqrt(d_model)\n",
    "        self.W_K = rng.standard_normal((d_model, d_model)) / np.sqrt(d_model)\n",
    "        self.W_V = rng.standard_normal((d_model, d_model)) / np.sqrt(d_model)\n",
    "        self.W_O = rng.standard_normal((d_model, d_model)) / np.sqrt(d_model)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def _split(self, x):\n",
    "        B,T, _=x.shape\n",
    "        return x.reshape(B, T, self.num_heads, self.d_head).transpose(0,2,1,3)\n",
    "    \n",
    "    def _merge(self, x):\n",
    "        B, H, T, dh = x.shape\n",
    "        return x.transpose(0,2,1,3).reshape(B, T, H*dh)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        B, T, d = x.shape\n",
    "        Q = x @ self.W_Q\n",
    "        K = x @ self.W_K\n",
    "        V = x @ self.W_V\n",
    "        Q = self._split(Q)\n",
    "        K = self._split(K)\n",
    "        V = self._split(V)\n",
    "        out, A = self.attention.forward(Q, K, V, mask)\n",
    "        out = self._merge(out) @ self.W_O\n",
    "        return out, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e24781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff, activation=\"gelu\", seed=99):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.W1 = rng.standard_normal((d_model, d_ff)) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros((d_ff,), dtype=np.float32)\n",
    "        self.W2 = rng.standard_normal((d_ff, d_model)) / np.sqrt(d_ff)\n",
    "        self.b2 = np.zeros((d_model,), dtype=np.float32)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x @ self.W1 + self.b1\n",
    "        if self.activation == \"gelu\":\n",
    "            h = gelu(h)\n",
    "        else:\n",
    "            h = np.maximum(0.0, h)  # ReLU\n",
    "        return h @ self.W2 + self.b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fea9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock:\n",
    "    def __init__(self,d_model,num_heads,d_ff, seed=100):\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads, seed=seed+1)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff, activation=\"gelu\", seed=seed+2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        a_in = self.ln1.forward(x)\n",
    "        a_out, A = self.attention.forward(a_in, mask)\n",
    "        x = x + a_out\n",
    "        f_in = self.ln2.forward(x)\n",
    "        f_out = self.ffn.forward(f_in)\n",
    "        x = x + f_out\n",
    "        return x, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef4b668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDecoder: \n",
    "    def __init__(self, vocab_size, max_len, d_model=128, num_heads=4, \n",
    "                 d_ff=512, num_layers=2, pos_encoding=\"sinusoidal\", seed=2024):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model, seed=seed)\n",
    "        self.pos_emb = PositionalEncoding(max_len, d_model, mode=pos_encoding, seed=seed+1)\n",
    "        self.blocks = [DecoderBlock(d_model, num_heads, d_ff, seed=seed+10*i) for i in range(num_layers)]\n",
    "        self.ln_finals = LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x_tokens, return_attentions=False):\n",
    "        B, T = x_tokens.shape\n",
    "        assert T <= self.max_len\n",
    "        x = self.tok_emb.forward(x_tokens)\n",
    "        x = x + self.pos_emb.forward(x)\n",
    "\n",
    "        mask = make_causal_mask(T)\n",
    "        attn_list = []\n",
    "\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, A = block.forward(x, mask)\n",
    "            if return_attentions:\n",
    "                attn_list.append(A)\n",
    "        \n",
    "        h = self.ln_finals.forward(x)\n",
    "\n",
    "        E = self.tok_emb.E\n",
    "        logits = h @ E.T\n",
    "\n",
    "        next_probs = softmax(logits[:, -1, :], axis=-1)\n",
    "\n",
    "        return (logits, next_probs, attn_list) if return_attentions else (logits, next_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e3ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (2, 16, 29)\n",
      "Next-token probs shape: (2, 29)\n",
      "Sum probs per sample: [1. 1.]\n",
      "Upper-triangle attention sum ~ 0: 0.0\n",
      "Sample 0 top-5 next tokens: [('e', 0.10356238858061897), ('g', 0.08674164074088994), ('f', 0.08216100729846752), ('w', 0.07069346943813608), ('h', 0.06532337093262273)]\n",
      "Sample 1 top-5 next tokens: [('e', 0.09836639580453611), ('a', 0.09055941651552268), ('g', 0.0889718299751409), ('f', 0.06864797769441527), ('w', 0.06552950037292793)]\n"
     ]
    }
   ],
   "source": [
    "class SimpleCharTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.chars = sorted(set(vocab))\n",
    "        self.stoi = {c:i for i,c in enumerate(self.chars)}\n",
    "        self.itos = {i:c for c,i in self.stoi.items()}\n",
    "    def encode(self, text):\n",
    "        return np.array([self.stoi[c] for c in text], dtype=np.int32)\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.itos[i] for i in indices])\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.chars)\n",
    "    \n",
    "# =====DEMO======\n",
    "if __name__ == \"__main__\":\n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz .,\"\n",
    "    tok = SimpleCharTokenizer(vocab)\n",
    "    V = tok.vocab_size()  \n",
    "\n",
    "    model = GPTDecoder(\n",
    "        vocab_size=V, max_len=32,\n",
    "        d_model=64, num_heads=4, d_ff=256, num_layers=2,\n",
    "        pos_encoding=\"sinusoidal\", seed=1234\n",
    "    )\n",
    "\n",
    "    def pad(ids, T):\n",
    "        out = np.zeros((T,), dtype=np.int32)\n",
    "        out[:min(T,len(ids))] = ids[:T]\n",
    "        return out\n",
    "\n",
    "    texts = [\"hello world.\", \"transformers, yay\"]\n",
    "    T = 16\n",
    "    x_tokens = np.stack([pad(tok.encode(t), T) for t in texts])  # [B,T]\n",
    "\n",
    "    logits, next_probs, attn_list = model.forward(x_tokens, return_attentions=True)  \n",
    "\n",
    "    print(\"Logits shape:\", logits.shape)         # [B,T,V]\n",
    "    print(\"Next-token probs shape:\", next_probs.shape)  # [B,V]\n",
    "    print(\"Sum probs per sample:\", next_probs.sum(axis=-1))  # ~ [1.0, 1.0]\n",
    "\n",
    "\n",
    "    aw = attn_list[0][0, 0]  \n",
    "    print(\"Upper-triangle attention sum ~ 0:\", float(np.triu(aw, k=1).sum()))\n",
    "\n",
    "\n",
    "    topk = 5\n",
    "    for b in range(x_tokens.shape[0]):\n",
    "        idxs = next_probs[b].argsort()[-topk:][::-1]\n",
    "        print(f\"Sample {b} top-{topk} next tokens:\",\n",
    "              [(tok.itos[i], float(next_probs[b, i])) for i in idxs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
